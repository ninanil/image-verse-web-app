{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":204160992,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport subprocess\nimport sys\n\n\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"diffusers\", \"bitsandbytes\", \"transformers\", \"fastapi\"])\n\n\nfrom argparse import Namespace\nimport math\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport torch\nfrom torch import autocast\nfrom torchvision import transforms\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, LMSDiscreteScheduler, DDPMScheduler\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nimport bitsandbytes as bnb\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nfrom fastapi import UploadFile\nfrom logger_config import logger\n\nclass StableDiffusionModel:\n    def __init__(self, height=512, width=512, num_inference_steps=50, guidance_scale=8, seed=64, batch_size=1):\n        logger.info(\"Initializing StableDiffusionModel with height=%d, width=%d, num_inference_steps=%d, guidance_scale=%f, seed=%d, batch_size=%d\", height, width, num_inference_steps, guidance_scale, seed, batch_size)\n        \n        self.height = height\n        self.width = width\n        self.num_inference_steps = num_inference_steps\n        self.guidance_scale = guidance_scale\n        self.generator = torch.manual_seed(seed)\n        self.batch_size = batch_size\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        # Load models\n        logger.info(\"Loading models...\")\n        self.vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(self.device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(self.device)\n        self.unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(self.device)\n        self.scheduler = PNDMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n\n    def latents_to_pil(self, latents):\n        logger.info(\"Converting latents to PIL images...\")\n        latents = (1 / 0.18215) * latents\n        with torch.no_grad():\n            image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n        images = (image * 255).round().astype(\"uint8\")\n        pil_images = [Image.fromarray(image) for image in images]\n        logger.info(\"Successfully converted latents to %d PIL images\", len(pil_images))\n        return pil_images\n\n    def prompt_to_emb(self, prompt, negative_prompts=''):\n        logger.info(\"Converting prompts to embeddings...\")\n        batch_size = len(prompt)\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=77,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        prompt_embeds = self.text_encoder(text_input_ids.to(self.device))[0]\n        prompt_embeds_dtype = self.text_encoder.dtype\n        prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=self.device)\n        _, seq_len, _ = prompt_embeds.shape\n        prompt_embeds = prompt_embeds.repeat(1, 1, 1).view(batch_size * 1, seq_len, -1)\n\n        negative_text_inputs = self.tokenizer(\n            negative_prompts,\n            padding=\"max_length\",\n            max_length=77,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        negative_input_ids = negative_text_inputs.input_ids\n        negative_prompt_embeds = self.text_encoder(negative_input_ids.to(self.device))[0]\n        negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=self.device)\n        _, seq_len, _ = negative_prompt_embeds.shape\n        negative_prompt_embeds = negative_prompt_embeds.repeat(1, 1, 1).view(batch_size * 1, seq_len, -1)\n\n        concatenated_embeddings = torch.cat([negative_prompt_embeds, prompt_embeds])\n        logger.info(\"Successfully converted prompts to embeddings\")\n        return concatenated_embeddings\n\n    def emb_to_latents(self, text_embeddings):\n        logger.info(\"Generating latents from embeddings...\")\n        self.scheduler.set_timesteps(self.num_inference_steps)\n        latents = torch.randn((1, 4, 64, 64), dtype=torch.float32).to(self.device)\n\n        for t in tqdm(self.scheduler.timesteps):\n            logger.debug(\"Processing timestep %s\", t)\n            latent_model_input = torch.cat([latents] * 2)\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n            with torch.no_grad():\n                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings, return_dict=False, added_cond_kwargs={'text_embeds': text_embeddings})[0]\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n            noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n\n        logger.info(\"Successfully generated latents\")\n        return latents\n\n\n\nclass ImageCaptioningPipeline:\n    def __init__(self, model_name=\"microsoft/Florence-2-large\"):\n        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\n        # Load the model and processor\n        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=self.torch_dtype, trust_remote_code=True).to(self.device)\n        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n\n    def load_image(self, file: UploadFile):\n        return Image.open(file.file)\n\n    def generate_text(self, prompt, max_new_tokens=1024, num_beams=3, do_sample=False):\n        # Load image\n        image = self.load_image()\n\n        # Preprocess inputs\n        inputs = self.processor(text=prompt, images=image, return_tensors=\"pt\").to(self.device, self.torch_dtype)\n\n        # Generate output\n        generated_ids = self.model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=max_new_tokens,\n            num_beams=num_beams,\n            do_sample=do_sample\n        )\n        generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n        # Post-process the generated text\n        parsed_answer = self.processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n        return parsed_answer","metadata":{"_uuid":"f1529e75-5612-42d0-a37b-5e36fde0f86a","_cell_guid":"6ffbffff-2396-44ca-a803-daeac2f63809","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-29T02:10:51.965778Z","iopub.execute_input":"2024-10-29T02:10:51.966639Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting diffusers\n  Downloading diffusers-0.31.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (7.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.25.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.4.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (10.3.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (2024.8.30)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.23.2->diffusers) (3.1.2)\nDownloading diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.31.0\n","output_type":"stream"}]}]}